{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/final_dataset_prepared_3_200_6000_1.csv\",converters={'tokenized': literal_eval})\n",
    "new = df[\"filename_path\"].str.split(\"/\",expand=True)\n",
    "df[\"filename_path\"] = './' + new[5] + '/' + new[6]+ '/' + new[7] + '/' + new[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the datset 29334\n",
      "length of train, validation and test dataset:\n",
      "29334 9778 9778\n",
      "total unique words in train, validation and test dataset:\n",
      "(5152,) (3399,) (3360,)\n",
      "saving train, validation and test dataset.....\n"
     ]
    }
   ],
   "source": [
    "df_train, df_dummy = train_test_split(df,random_state=202,test_size=0.40)\n",
    "df_val,df_test = train_test_split(df_dummy,random_state=202,test_size=0.50)\n",
    "\n",
    "print(\"length of the datset\",df_train.shape[0])\n",
    "\n",
    "print(\"length of train, validation and test dataset:\")\n",
    "print(df_train.shape[0],df_val.shape[0],df_test.shape[0])\n",
    "\n",
    "print(\"total unique words in train, validation and test dataset:\")\n",
    "print(df_train[\"word\"].unique().shape,df_val[\"word\"].unique().shape,df_test[\"word\"].unique().shape)\n",
    "\n",
    "print(\"saving train, validation and test dataset.....\")\n",
    "\n",
    "df_train.to_csv(\"../data/train.csv\",index=False)\n",
    "df_val.to_csv(\"../data/val.csv\",index=False)\n",
    "df_test.to_csv(\"../data/test.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>flag</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>word</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>count_tokens</th>\n",
       "      <th>sp_id_and_ch_id</th>\n",
       "      <th>filename_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSP-130776-0033_1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>DOTED</td>\n",
       "      <td>[do, t, ed]</td>\n",
       "      <td>3</td>\n",
       "      <td>-130776-0033</td>\n",
       "      <td>./train-other-500/1171/130776/1171-130776-0033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSP-12369-0048_1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>FETA</td>\n",
       "      <td>[fe, t, a]</td>\n",
       "      <td>3</td>\n",
       "      <td>-12369-0048</td>\n",
       "      <td>./train-other-500/4042/12369/4042-12369-0048.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSP-130296-0000_1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.60</td>\n",
       "      <td>THORD</td>\n",
       "      <td>[th, or, d]</td>\n",
       "      <td>3</td>\n",
       "      <td>-130296-0000</td>\n",
       "      <td>./train-clean-360/606/130296/606-130296-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSP-129393-0052_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.61</td>\n",
       "      <td>MONGST</td>\n",
       "      <td>[m, ong, st]</td>\n",
       "      <td>3</td>\n",
       "      <td>-129393-0052</td>\n",
       "      <td>./train-clean-360/708/129393/708-129393-0052.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSP-129443-0099_1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.52</td>\n",
       "      <td>PALME</td>\n",
       "      <td>[p, al, me]</td>\n",
       "      <td>3</td>\n",
       "      <td>-129443-0099</td>\n",
       "      <td>./train-clean-360/1811/129443/1811-129443-0099...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  flag  start  duration    word     tokenized  \\\n",
       "0  LSP-130776-0033_1     1   8.09      0.51   DOTED   [do, t, ed]   \n",
       "1   LSP-12369-0048_1     1   3.64      0.63    FETA    [fe, t, a]   \n",
       "2  LSP-130296-0000_1     1   4.81      0.60   THORD   [th, or, d]   \n",
       "3  LSP-129393-0052_1     1   0.11      0.61  MONGST  [m, ong, st]   \n",
       "4  LSP-129443-0099_1     1  12.83      0.52   PALME   [p, al, me]   \n",
       "\n",
       "   count_tokens sp_id_and_ch_id  \\\n",
       "0             3    -130776-0033   \n",
       "1             3     -12369-0048   \n",
       "2             3    -130296-0000   \n",
       "3             3    -129393-0052   \n",
       "4             3    -129443-0099   \n",
       "\n",
       "                                       filename_path  \n",
       "0  ./train-other-500/1171/130776/1171-130776-0033...  \n",
       "1  ./train-other-500/4042/12369/4042-12369-0048.flac  \n",
       "2  ./train-clean-360/606/130296/606-130296-0000.flac  \n",
       "3  ./train-clean-360/708/129393/708-129393-0052.flac  \n",
       "4  ./train-clean-360/1811/129443/1811-129443-0099...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48139,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append the \"word\" column to the \"filename_path\" column, and \"start\" column and count the unique entries in the resulting column\n",
    "df[\"filename_path\"] = df[\"filename_path\"] + \"_\" + df[\"word\"] #+ \"_\" + df[\"start\"].astype(str)\n",
    "df[\"filename_path\"].unique().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./train-clean-360/2512/157242/2512-157242-0000.flac_HOORAY_HOORAY      4\n",
       "./train-other-500/3926/174988/3926-174988-0039.flac_ENEMY_ENEMY        4\n",
       "./train-other-500/228/133094/228-133094-0016.flac_FAIRY_FAIRY          4\n",
       "./train-clean-100/1743/142912/1743-142912-0031.flac_WREN_WREN          4\n",
       "./train-other-500/5733/55099/5733-55099-0037.flac_THRIVE_THRIVE        3\n",
       "                                                                      ..\n",
       "./train-other-500/3500/134409/3500-134409-0064.flac_MORSEL_MORSEL      1\n",
       "./train-other-500/6733/74807/6733-74807-0031.flac_REUNION_REUNION      1\n",
       "./train-other-500/5044/30602/5044-30602-0010.flac_ROTATION_ROTATION    1\n",
       "./train-clean-360/7339/86765/7339-86765-0058.flac_NOTARY_NOTARY        1\n",
       "./train-clean-360/1509/145742/1509-145742-0011.flac_BANG_BANG          1\n",
       "Name: filename_path, Length: 48139, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the repetition count of each unique entry in the \"filename_path\" column\n",
    "df[\"filename_path\"].str.cat(df[\"word\"],sep=\"_\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['word']=='HIMSELF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_awe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
